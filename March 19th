•	Bagging is a special case of random forests under which case?
    When the subset of the predictors is the total number of predictors. 
    Bagging is a special case of random forest with m = p (total number of predictors)
    
•	What are the hyperparameters we can control for random forests?
    The depth of the tree, how many subsets we create and how many predictors we devise in each subset

•	Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
    i.	(1,0), (1,2), (1,5)  Invalid: One data is a different observation 
    ii.	(1,2), (2,0)  Invalid: because it is the sample of the original data but one is missing
    iii.	(1,2), (1,2), (1,5)   valid: we can have repeated samples 

•	For each of the above valid bootstrapped data sets, which observations are out-of-bag (OOB)?  
    iii: (2,0)

•	You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
    i.	Regression: your trees make the following four predictions: 1,1,3,3.
    2 – average of predictions
    ii.	Classification: your trees make the following four predictions: "A", "A", "B", "C".
    A - mode
